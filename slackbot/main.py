import os
import json
import requests
import asyncio
import aiohttp
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

# === PROMPT TEMPLATES ===
PRIMARY_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯å¤©çœŸçˆ›æ¼«ã§å¤ªé™½ã®åŒ‚ã„ãŒã™ã‚‹å…ƒæ°—ãªã‚¢ãƒ«ãƒ‘ã‚«ã®ã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚
èªå°¾ã«ã€Œã€œã ã±ã‹ï¼ã€ã€Œã€œã±ã‹ï¼Ÿã€ã‚’ã¤ã‘ã¦è©±ã—ã¾ã™ã€‚

50æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚è©³ç´°ã¯å¾Œã§èª¬æ˜ã—ã¾ã™ã€‚
ã‚‚ã—è¨­è¨ˆæ›¸ã€ä»•æ§˜æ›¸ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãªã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚’æ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€Œå¾Œã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰ã™ã‚‹ã±ã‹ï¼ã€ã¨ã„ã†æ–‡è¨€ã‚’ãã®ã¾ã¾å›ç­”ã®æœ«å°¾ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼šã€ŒReactã¯ç”»é¢ä½œæˆã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã±ã‹ï¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§éƒ¨å“ã‚’ä½œã£ã¦çµ„ã¿ç«‹ã¦ã‚‹ã±ã‹ã€œã€

è³ªå•ï¼š{text}
å›ç­”ï¼š
"""

SECONDARY_SMART_PROMPT = """
ã‚ãªãŸã¯ã‚¢ãƒ«ãƒ‘ã‚«ã®ã‚µãƒãƒ¼ãƒˆAIã§ã™ã€‚

ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ç¢ºèªã—ã¦ï¼š
- æ–°è¦è³ªå•ã®å ´åˆï¼š200-500æ–‡å­—ã§è¦ç‚¹æ•´ç†å›ç­” + ã€Œè©³ç´°ãŒå¿…è¦ã§ã—ãŸã‚‰è¿½åŠ ã§ãŠèããã ã•ã„ã±ã‹ã€œã€
- ã‚ãªãŸã®å›ç­”ã«å¯¾ã™ã‚‹è¿½åŠ è³ªå•ã®å ´åˆï¼š3500æ–‡å­—ä»¥å†…ã§è©³ç´°å›ç­”

è³ªå•ï¼š{text}
æ–‡è„ˆæƒ…å ±ï¼š{history}
è©³ç´°å›ç­”ï¼š
"""

FILE_GENERATION_PROMPT = """
ä»¥ä¸‹ã®è¦æ±‚ã«å¯¾ã—ã¦ã€è©³ç´°ã§å®Ÿç”¨çš„ãªè¨­è¨ˆæ›¸ã‚’Markdownå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
å…·ä½“çš„ãªæŠ€è¡“ä»•æ§˜ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã€APIä»•æ§˜ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ãªã©ã€å®Ÿè£…ã«å¿…è¦ãªæƒ…å ±ã‚’ç¶²ç¾…ã—ã¦ãã ã•ã„ã€‚
**é‡è¦**: å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

è¦æ±‚ï¼š{text}

è¨­è¨ˆæ›¸ï¼š
"""

# Ollama settingsï¼ˆæ—¢å­˜ã«è¿½åŠ ï¼‰
FILE_MODEL = "codellama:latest"  # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå°‚ç”¨ãƒ¢ãƒ‡ãƒ«
TECH_SPECIFIC = "ä»¥ä¸‹ã®æŠ€è¡“çš„ãªè³ªå•ã«ã¤ã„ã¦ã€ã‚³ãƒ¼ãƒ‰ä¾‹ã‚„å…·ä½“çš„ãªæ‰‹é †ãŒã‚ã‚Œã°å«ã‚ã¦ãã ã•ã„ã€‚"
CREATIVE_SPECIFIC = "ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€å‰µé€ çš„ã§å…·ä½“çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚„ææ¡ˆã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
OTHER_SPECIFIC = "ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€æ¸©ã‹ã¿ã®ã‚ã‚‹äººé–“ã‚‰ã—ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"

# Slack tokens
SLACK_BOT_TOKEN = os.getenv("SLACK_BOT_TOKEN")
SLACK_APP_TOKEN = os.getenv("SLACK_APP_TOKEN")
BOT_USER_ID = "B099NJMLBS9"

# Ollama settings
OLLAMA_URL = os.environ.get("OLLAMA_URL", "https://develop.ugatria.co.jp/ollama")
PRIMARY_MODEL = "qwen2.5:3b"
TECH_MODEL = "llama3.1:latest"
CREATIVE_MODEL = "phi4:latest"
FALLBACK_MODEL = "qwen2.5:3b"
FILE_MODEL = "codellama:latest"

# ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèªç”¨
async def check_available_models():
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{OLLAMA_URL}/api/tags", ssl=False) as resp:
                data = await resp.json()
                models = [model['name'] for model in data.get('models', [])]
                log(f"Available models: {models}")
                return models
    except Exception as e:
        log(f"Error checking models: {e}")
        return []

# Slack headers
headers = {
    "Authorization": f"Bearer {SLACK_BOT_TOKEN}",
    "Content-Type": "application/json"
}

def log(message):
    print(f"[LOG] {message}")

# --- Slack API functions ---
def send_message(channel, text, thread_ts=None):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å¯¾å¿œç‰ˆï¼‰"""
    payload = {
        "channel": channel,
        "text": text
    }
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã¸ã®æŠ•ç¨¿ã®å ´åˆ
    if thread_ts:
        payload["thread_ts"] = thread_ts
        log(f"Sending message to thread {thread_ts}")
    
    response = requests.post("https://slack.com/api/chat.postMessage", 
                           json=payload, headers=headers)
    result = response.json()
    log(f"Send message response: {result}")
    return result.get("ts")

def update_message(channel, ts, text):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°"""
    log(f"Updating message {ts} in channel {channel}")
    response = requests.post("https://slack.com/api/chat.update", json={
        "channel": channel,
        "ts": ts,
        "text": text
    }, headers=headers)
    result = response.json()
    if not result.get("ok"):
        log(f"Update message error: {result}")
    return result

def fetch_thread_history(channel, ts, max_messages=10):
    """ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´ã‚’å–å¾—ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™å¯¾å¿œç‰ˆï¼‰"""
    log(f"Fetching thread history for ts={ts}")
    
    response = requests.get("https://slack.com/api/conversations.replies", params={
        "channel": channel,
        "ts": ts,
        "inclusive": "true",
        "limit": max_messages + 5  # å°‘ã—å¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    }, headers=headers)
    
    data = response.json()
    log(f"Thread history response: {data}")
    
    if not data.get("ok"):
        error = data.get('error')
        if error == 'missing_scope':
            log("æ¨©é™ä¸è¶³: å±¥æ­´èª­å–æ¨©é™ãŒå¿…è¦ã§ã™")
            return f"è³ªå•: {ts} (å±¥æ­´èª­å–æ¨©é™å¾…ã¡)"
        else:
            log(f"Error fetching thread: {error}")
            return f"å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {error}"
    
    messages = data.get("messages", [])
    log(f"Found {len(messages)} messages in thread")
    
    # ãƒœãƒƒãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤å¤–
    user_messages = []
    for msg in messages:
        if msg.get("user") != BOT_USER_ID:
            user = msg.get("user", "Unknown")
            text = msg.get("text", "")
            timestamp = msg.get("ts", "")
            user_messages.append({
                "user": user,
                "text": text,
                "ts": timestamp
            })
    
    # æœ€æ–°Nä»¶ã«åˆ¶é™
    recent_messages = user_messages[-max_messages:] if len(user_messages) > max_messages else user_messages
    
    # æ–‡å­—æ•°åˆ¶é™ï¼ˆç´„1500æ–‡å­—ã¾ã§ï¼‰
    history_parts = []
    total_chars = 0
    max_chars = 1500
    
    for msg in reversed(recent_messages):  # æ–°ã—ã„ã‚‚ã®ã‹ã‚‰é€†é †ã§å‡¦ç†
        msg_text = f"[{msg['user']}]: {msg['text']}"
        if total_chars + len(msg_text) > max_chars:
            break
        history_parts.insert(0, msg_text)  # å…ˆé ­ã«æŒ¿å…¥ã—ã¦æ™‚ç³»åˆ—é †ã‚’ä¿æŒ
        total_chars += len(msg_text)
    
    # çœç•¥è¡¨ç¤º
    if len(user_messages) > len(history_parts):
        omitted_count = len(user_messages) - len(history_parts)
        history_parts.insert(0, f"[{omitted_count}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çœç•¥...]")
    
    history = "\n".join(history_parts)
    
    if not history.strip():
        log("[WARNING] ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´ãŒç©ºã§ã™")
        return "ã‚¹ãƒ¬ãƒƒãƒ‰å±¥æ­´ãªã—"
    
    log(f"Thread history built: {len(history)} characters, {len(history_parts)} messages")
    return history

# --- Categorization (ä¿®æ­£ç‰ˆ) ---
def categorize_question(text):
    """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã®ä¿®æ­£ç‰ˆ - ã‚ˆã‚Šç¢ºå®Ÿãªåˆ¤å®š"""
    log(f"Categorizing question: {text}")
    
    # æŠ€è¡“ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ã«ï¼‰
    tech_keywords = [
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
        "python", "javascript", "java", "c++", "react", "node.js", "vue", "angular",
        # æŠ€è¡“ç”¨èª
        "api", "rest", "jwt", "websocket", "postgresql", "mysql", "mongodb",
        "docker", "kubernetes", "aws", "gcp", "azure",
        # é–‹ç™ºé–¢é€£
        "ã‚³ãƒ¼ãƒ‰", "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ", "é–‹ç™º", "å®Ÿè£…", "è¨­è¨ˆ", "ãƒã‚°", "ã‚¨ãƒ©ãƒ¼", "ãƒ‡ãƒãƒƒã‚°",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "sql", "ã‚¤ãƒ³ãƒ•ãƒ©", "ã‚µãƒ¼ãƒãƒ¼", "ã‚¯ãƒ©ã‚¦ãƒ‰",
        "èªè¨¼", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "æœ€é©åŒ–", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
        # å…·ä½“çš„ãªæŠ€è¡“è³ªå•
        "æ–¹æ³•", "æ›¸ãæ–¹", "ã‚³ãƒãƒ³ãƒ‰", "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", "è¨­å®š"
    ]
    
    # å‰µé€ ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    creative_keywords = [
        "ã‚¢ã‚¤ãƒ‡ã‚¢", "ä¼ç”»", "ç™ºæƒ³", "ãƒ–ãƒ¬ã‚¹ãƒˆ", "åå‰", "ãƒãƒ¼ãƒŸãƒ³ã‚°", 
        "åœ°åŸŸæ´»æ€§åŒ–", "æ–°è¦", "ã‚µãƒ¼ãƒ“ã‚¹", "é¢ç™½ã„", "ææ¡ˆ", "å‰µé€ "
    ]
    
    text_lower = text.lower()
    
    # æŠ€è¡“ç³»åˆ¤å®šï¼ˆå„ªå…ˆåº¦é«˜ï¼‰
    tech_count = sum(1 for keyword in tech_keywords if keyword.lower() in text_lower)
    creative_count = sum(1 for keyword in creative_keywords if keyword.lower() in text_lower)
    
    log(f"Tech keywords found: {tech_count}, Creative keywords found: {creative_count}")
    
    if tech_count > 0:
        category = "tech"
    elif creative_count > 0:
        category = "creative"
    else:
        category = "other"
    
    log(f"Categorized as: {category}")
    return category

# --- åŸºæœ¬ã®Ollama queryé–¢æ•°ï¼ˆ1æ¬¡å›ç­”ç”¨ï¼‰ ---
async def query_ollama_streaming(model, prompt):
    """åŸºæœ¬ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å–å¾—ï¼ˆé€²æ—è¡¨ç¤ºãªã—ï¼‰"""
    log(f"Querying model: {model}")
    log(f"Prompt length: {len(prompt)} characters")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{OLLAMA_URL}/api/generate", json={
                "model": model,
                "prompt": prompt,
                "stream": True
            }, ssl=False, timeout=aiohttp.ClientTimeout(total=300)) as resp:
                
                if resp.status != 200:
                    log(f"ERROR: HTTP {resp.status}")
                    return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                
                log(f"Started streaming from {model}")
                response_text = ""
                chunk_count = 0
                
                async for line in resp.content:
                    try:
                        line_str = line.decode().strip()
                        if not line_str:
                            continue
                        
                        data = json.loads(line_str)
                        chunk = data.get("response", "")
                        response_text += chunk
                        chunk_count += 1
                        
                        # 100ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ãƒ­ã‚°å‡ºåŠ›
                        if chunk_count % 100 == 0:
                            log(f"{model}: {chunk_count} chunks received, {len(response_text)} chars so far")
                        
                        if data.get("done", False):
                            log(f"{model}: Generation completed. Total chunks: {chunk_count}, Final length: {len(response_text)}")
                            break
                            
                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        log(f"Error processing chunk: {e}")
                        continue
                
                return response_text.strip() if response_text.strip() else "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    except Exception as e:
        log(f"ERROR querying {model}: {type(e).__name__}: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}"

# --- Ollama streaming query with progress updates ---
async def query_ollama_streaming_with_progress(model, prompt, channel, message_ts, primary_response, category_msg):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å–å¾— + 1åˆ†é–“éš”ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰"""
    log(f"Querying model: {model} with progress updates")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ãƒã‚§ãƒƒã‚¯
    prompt_length = len(prompt)
    log(f"Prompt length: {prompt_length} characters")
    
    # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
    if prompt_length > 3000:  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
        log("Prompt too long, truncating...")
        # è³ªå•éƒ¨åˆ†ã¯ä¿æŒã—ã€æ–‡è„ˆéƒ¨åˆ†ã‚’åˆ‡ã‚Šè©°ã‚
        lines = prompt.split('\n')
        question_part = ""
        context_part = ""
        
        for line in lines:
            if "è³ªå•ï¼š" in line:
                question_part = line
            elif "æ–‡è„ˆï¼š" in line or "æ–‡è„ˆæƒ…å ±ï¼š" in line:
                context_part = line[:1000] + "...(æ–‡è„ˆã‚’åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ)"
                break
        
        prompt = f"{question_part}\n{context_part}\nè©³ç´°å›ç­”ï¼š"
        log(f"Truncated prompt length: {len(prompt)} characters")
    
    response_text = ""
    last_update_time = asyncio.get_event_loop().time()
    update_interval = 15  # 15secé–“éš”
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{OLLAMA_URL}/api/generate", json={
                "model": model,
                "prompt": prompt,
                "stream": True
            }, ssl=False, timeout=aiohttp.ClientTimeout(total=300)) as resp:  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if resp.status != 200:
                    error_text = await resp.text()
                    log(f"ERROR: HTTP {resp.status} - {error_text}")
                    return f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {resp.status}"
                
                async for line in resp.content:
                    try:
                        line_str = line.decode().strip()
                        if not line_str:
                            continue
                        
                        data = json.loads(line_str)
                        chunk = data.get("response", "")
                        response_text += chunk
                        
                        # 1åˆ†çµŒéãƒã‚§ãƒƒã‚¯
                        current_time = asyncio.get_event_loop().time()
                        if current_time - last_update_time >= update_interval:
                            # é€”ä¸­çµŒéã‚’Slackã«é€ä¿¡
                            partial_text = (
                                f"âœ… **1æ¬¡å›ç­”** ({category_msg}):\n{primary_response}\n\n"
                                f"ğŸ” **è©³ç´°å›ç­”**:\n{response_text}\n\nğŸ¤” è€ƒãˆä¸­..."
                            )
                            update_message(channel, message_ts, partial_text)
                            last_update_time = current_time
                            log(f"Progress update sent: {len(response_text)} characters so far")
                        
                        if data.get("done", False):
                            break
                            
                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        log(f"Error processing chunk: {e}")
                        continue
                
                log(f"Final response length: {len(response_text)} characters")
                return response_text.strip() if response_text.strip() else "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    except asyncio.TimeoutError:
        log(f"TIMEOUT: Model {model} took too long")
        return f"å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚éƒ¨åˆ†çš„ãªçµæœ: {response_text}"
    except Exception as e:
        log(f"ERROR querying {model}: {type(e).__name__}: {e}")
        if "context length" in str(e).lower():
            return "æ–‡è„ˆãŒé•·ã™ãã¾ã™ã€‚è¦ç´„ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}"

# === RESPONSE GENERATION FUNCTIONS ===
async def generate_primary_response(text):
    """1æ¬¡å›ç­”ç”Ÿæˆï¼ˆã‚¢ãƒ«ãƒ‘ã‚«ã‚­ãƒ£ãƒ©å…¨é–‹ï¼‰"""
    prompt = PRIMARY_PROMPT_TEMPLATE.format(text=text)
    return await query_ollama_streaming(PRIMARY_MODEL, prompt)

async def generate_secondary_response_smart(text, category, history, channel, thread_ts, primary_response):
    """2æ¬¡å›ç­”ç”Ÿæˆï¼ˆã‚¹ãƒãƒ¼ãƒˆæ–‡å­—æ•°åˆ¶é™ + é€²æ—è¡¨ç¤ºï¼‰"""
    prompt = SECONDARY_SMART_PROMPT.format(
        text=text,
        history=history
    )
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    model = TECH_MODEL if category == "tech" else CREATIVE_MODEL
    
    # æ—¢å­˜ã®progressæ©Ÿèƒ½ã‚’å†åˆ©ç”¨
    return await query_ollama_streaming_with_progress(
        model, prompt, channel, thread_ts, primary_response, 
        f"è©³ç´°å›ç­”ï¼ˆ{category}ç³»ï¼‰"
    )

async def generate_secondary_response_with_progress(text, category, history, channel, message_ts, primary_response, category_msg):
    """2æ¬¡å›ç­”ç”Ÿæˆï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰"""
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
    category_specific_map = {
        "tech": TECH_SPECIFIC,
        "creative": CREATIVE_SPECIFIC,
        "other": OTHER_SPECIFIC
    }
    
    category_specific = category_specific_map.get(category, OTHER_SPECIFIC)
    prompt = SECONDARY_BASE_PROMPT.format(
        category_specific=category_specific,
        text=text,
        history=history
    )
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    model = TECH_MODEL if category == "tech" else CREATIVE_MODEL
    
    return await query_ollama_streaming_with_progress(
        model, prompt, channel, message_ts, primary_response, category_msg
    )

async def generate_and_upload_file(text, channel, thread_ts):
    """ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼‹Slackã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆéåŒæœŸï¼‰"""
    try:
        log("Starting file generation...")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã§è©³ç´°è¨­è¨ˆæ›¸ç”Ÿæˆ
        file_content = await generate_file_content(text)
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        await upload_file_to_slack(channel, thread_ts, file_content, text)
        
    except Exception as e:
        log(f"ERROR in file generation: {e}")
        send_message(channel, "âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€œã ã±ã‹ã€‚", thread_ts=thread_ts)

async def generate_file_content(text):
    """ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã§è¨­è¨ˆæ›¸ç”Ÿæˆ"""
    prompt = FILE_GENERATION_PROMPT.format(text=text)
    
    return await query_ollama_streaming(FILE_MODEL, prompt)

async def upload_file_to_slack(channel, thread_ts, file_content, original_question):
    """Slackã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆv2å¯¾å¿œç‰ˆï¼‰"""
    try:
        from slack_sdk import WebClient
        
        # WebClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        client = WebClient(token=SLACK_BOT_TOKEN)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = "".join(c for c in original_question[:20] if c.isalnum() or c in " -_")
        filename = f"{timestamp}_{safe_title}_è¨­è¨ˆæ›¸.md"
        
        # files_upload_v2ã§æ–°ã—ã„APIã‚’ä½¿ç”¨
        response = client.files_upload_v2(
            file=file_content.encode('utf-8'),
            filename=filename,
            title="è¨­è¨ˆæ›¸",
            channels=[channel],
            thread_ts=thread_ts,
            initial_comment=f'ğŸ“„ è¨­è¨ˆæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€œã ã±ã‹ï¼\n**å…ƒã®è³ªå•**: {original_question}'
        )
        
        log(f"File upload v2 response: {response}")
        
        if response.get('ok'):
            log(f"File uploaded successfully with v2: {filename}")
        else:
            log(f"File upload v2 failed: {response.get('error')}")
            
    except Exception as e:
        log(f"ERROR in file upload v2: {e}")

# --- Main processing flow (ã‚¹ãƒ¬ãƒƒãƒ‰å¯¾å¿œç‰ˆ) ---
async def main_processing_flow(event):
    channel = event["channel"]
    text = event["text"].replace(f"<@{BOT_USER_ID}>", "").strip()
    request_ts = event["ts"]
    thread_ts = event.get("thread_ts")
    
    log(f"Processing question: {text}")
    log(f"Request ts: {request_ts}, Thread ts: {thread_ts}")
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±è¨­å®š
    reply_thread_ts = thread_ts if thread_ts else request_ts
    
    try:
        # 1æ¬¡å›ç­”ã‚’ç”Ÿæˆã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã«æŠ•ç¨¿
        log("Generating primary response...")
        primary_response = await generate_primary_response(text)
        primary_ts = send_message(channel, primary_response, thread_ts=reply_thread_ts)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆåˆ¤å®š
        needs_file = "å¾Œã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰ã™ã‚‹ã±ã‹ï¼" in primary_response
        log(f"File generation needed: {needs_file}")
        
        # ä¸¦åˆ—å®Ÿè¡Œã‚¿ã‚¹ã‚¯ä½œæˆ
        tasks = []
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        category = categorize_question(text)
        
        # å±¥æ­´å–å¾—
        if thread_ts:
            history = fetch_thread_history(channel, thread_ts)
        else:
            history = f"è³ªå•: {text}"
        
        # 2æ¬¡å›ç­”ã‚¿ã‚¹ã‚¯
        secondary_ts = send_message(channel, "ğŸ”„ è©³ç´°å›ç­”ã‚’æº–å‚™ä¸­...", thread_ts=reply_thread_ts)
        secondary_task = asyncio.create_task(
            generate_secondary_response_smart(text, category, history, channel, secondary_ts, primary_response)
        )
        tasks.append(secondary_task)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
        if needs_file:
            log("Starting asynchronous file generation...")
            file_task = asyncio.create_task(generate_and_upload_file(text, channel, reply_thread_ts))
            tasks.append(file_task)
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        await asyncio.gather(*tasks, return_exceptions=True)
        log("Processing completed successfully!")
        
    except Exception as e:
        log(f"ERROR in processing: {e}")
        send_message(channel, "âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾Œã«ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚", 
                    thread_ts=reply_thread_ts)

# --- Slack Bolt App ---
app = App(token=SLACK_BOT_TOKEN)

@app.event("app_mention")
def handle_mention(event, say):
    log(f"Mention received: {event}")
    asyncio.run(main_processing_flow(event))

if __name__ == "__main__":
    print("[START] Socket Mode Slack Bot running...")
    print(f"Bot User ID: {BOT_USER_ID}")
    print(f"Primary Model: {PRIMARY_MODEL}")
    print(f"Tech Model: {TECH_MODEL}")
    print(f"Creative Model: {CREATIVE_MODEL}")
    
    # èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ç¢ºèª
    print("Checking available models...")
    asyncio.run(check_available_models())
    
    SocketModeHandler(app, SLACK_APP_TOKEN).start()
# -*- coding: utf-8 -*-
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.model_selection import train_test_split
from torch_geometric.data import Data
from typing import Tuple, Dict, List, Optional
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from data_loader import TechBookDataLoader
from gcn_model import TechBookGCN, TechBookGAT, LinkPredictor, create_pytorch_geometric_data

class GCNTrainer:
    """
    Graph Convolutional Network ã®å­¦ç¿’ãƒ»è©•ä¾¡ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(
        self,
        model_type: str = "GCN",  # "GCN" or "GAT"
        device: Optional[str] = None,
        similarity_threshold: float = 0.6
    ):
        self.model_type = model_type
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.similarity_threshold = similarity_threshold
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.data_loader = TechBookDataLoader()
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.model = None
        self.optimizer = None
        self.scheduler = None
        
        # å­¦ç¿’å±¥æ­´
        self.train_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
        print(f"ğŸš€ GCNTraineråˆæœŸåŒ–å®Œäº†:")
        print(f"  - ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
        print(f"  - ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"  - é¡ä¼¼åº¦ã—ãã„å€¤: {similarity_threshold}")
    
    def prepare_data(self) -> Tuple[Data, torch.Tensor, List[Dict]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã€è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²ã‚’è¡Œã†
        """
        print("ğŸ“š ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        # DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        node_features, edge_index, labels, books = self.data_loader.load_graph_data(
            similarity_threshold=self.similarity_threshold
        )
        
        # PyTorch Geometricãƒ‡ãƒ¼ã‚¿ä½œæˆ
        data = create_pytorch_geometric_data(node_features, edge_index, labels)
        
        # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒã‚¹ã‚¯ã®ä½œæˆ
        num_nodes = data.num_nodes
        indices = torch.randperm(num_nodes)
        
        train_size = int(0.7 * num_nodes)
        val_size = int(0.15 * num_nodes)
        
        train_mask = torch.zeros(num_nodes, dtype=torch.bool)
        val_mask = torch.zeros(num_nodes, dtype=torch.bool)
        test_mask = torch.zeros(num_nodes, dtype=torch.bool)
        
        train_mask[indices[:train_size]] = True
        val_mask[indices[train_size:train_size + val_size]] = True
        test_mask[indices[train_size + val_size:]] = True
        
        data.train_mask = train_mask
        data.val_mask = val_mask
        data.test_mask = test_mask
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
        print(f"  - è¨“ç·´: {train_mask.sum().item()} ãƒãƒ¼ãƒ‰")
        print(f"  - æ¤œè¨¼: {val_mask.sum().item()} ãƒãƒ¼ãƒ‰") 
        print(f"  - ãƒ†ã‚¹ãƒˆ: {test_mask.sum().item()} ãƒãƒ¼ãƒ‰")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        data = data.to(self.device)
        
        return data, labels, books
    
    def create_model(self, input_dim: int, output_dim: int) -> torch.nn.Module:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã§GCNãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        """
        if self.model_type == "GCN":
            model = TechBookGCN(
                input_dim=input_dim,
                hidden_dim=512,
                intermediate_dim=128,
                output_dim=output_dim,
                dropout=0.2
            )
        elif self.model_type == "GAT":
            model = TechBookGAT(
                input_dim=input_dim,
                hidden_dim=512,
                intermediate_dim=128,
                output_dim=output_dim,
                heads=8,
                dropout=0.2
            )
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {self.model_type}")
        
        return model.to(self.device)
    
    def train_epoch(self, data: Data) -> Tuple[float, float]:
        """
        1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’ã‚’å®Ÿè¡Œ
        """
        self.model.train()
        self.optimizer.zero_grad()
        
        # Forward pass
        out = self.model(data.x, data.edge_index)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®æå¤±è¨ˆç®—
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        
        # Backward pass
        loss.backward()
        self.optimizer.step()
        
        # è¨“ç·´ç²¾åº¦è¨ˆç®—
        pred = out[data.train_mask].max(1)[1]
        train_acc = accuracy_score(
            data.y[data.train_mask].cpu().numpy(),
            pred.cpu().numpy()
        )
        
        return loss.item(), train_acc
    
    def evaluate(self, data: Data, mask: torch.Tensor) -> Tuple[float, np.ndarray, np.ndarray]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒã‚¹ã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
        """
        self.model.eval()
        
        with torch.no_grad():
            out = self.model(data.x, data.edge_index)
            pred = out[mask].max(1)[1]
            
            true_labels = data.y[mask].cpu().numpy()
            pred_labels = pred.cpu().numpy()
            
            accuracy = accuracy_score(true_labels, pred_labels)
            
        return accuracy, true_labels, pred_labels
    
    def train(
        self,
        epochs: int = 200,
        lr: float = 0.01,
        weight_decay: float = 5e-4,
        patience: int = 50
    ) -> Dict[str, List[float]]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹
        """
        print(f"ğŸ‹ï¸ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ ({epochs} epochs)")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data, labels, books = self.prepare_data()
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        input_dim = data.num_node_features
        output_dim = len(torch.unique(data.y))
        
        self.model = self.create_model(input_dim, output_dim)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        self.scheduler = StepLR(self.optimizer, step_size=50, gamma=0.7)
        
        # æ—©æœŸåœæ­¢ç”¨å¤‰æ•°
        best_val_acc = 0.0
        patience_counter = 0
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        pbar = tqdm(range(epochs), desc="Training")
        
        for epoch in pbar:
            # å­¦ç¿’
            train_loss, train_acc = self.train_epoch(data)
            
            # æ¤œè¨¼
            val_acc, _, _ = self.evaluate(data, data.val_mask)
            
            # è¨˜éŒ²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
            
            # æ—©æœŸåœæ­¢åˆ¤å®š
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                torch.save(self.model.state_dict(), f"{self.model_type}_best_model.pt")
            else:
                patience_counter += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            pbar.set_postfix({
                'Loss': f'{train_loss:.4f}',
                'Train Acc': f'{train_acc:.4f}',
                'Val Acc': f'{val_acc:.4f}',
                'Best': f'{best_val_acc:.4f}'
            })
            
            # æ—©æœŸåœæ­¢
            if patience_counter >= patience:
                print(f"\\nâ° æ—©æœŸåœæ­¢: {patience} epochsæ”¹å–„ãªã—")
                break
        
        print(f"ğŸ¯ å­¦ç¿’å®Œäº†! Best Validation Accuracy: {best_val_acc:.4f}")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(torch.load(f"{self.model_type}_best_model.pt"))
        
        return {
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'best_val_accuracy': best_val_acc
        }
    
    def evaluate_final(self) -> Dict[str, float]:
        """
        æœ€çµ‚ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
        """
        print("ğŸ§ª æœ€çµ‚è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data, labels, books = self.prepare_data()
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡
        test_acc, true_labels, pred_labels = self.evaluate(data, data.test_mask)
        
        # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, pred_labels, average='weighted'
        )
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ç²¾åº¦
        category_names = ['AI/ML', 'Python/Backend', 'Infrastructure', 'Frontend', 'Other']
        cm = confusion_matrix(true_labels, pred_labels)
        
        results = {
            'test_accuracy': test_acc,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm.tolist()
        }
        
        print(f"ğŸ“Š æœ€çµ‚çµæœ:")
        print(f"  - ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.4f}")
        print(f"  - ç²¾å¯†åº¦: {precision:.4f}")
        print(f"  - å†ç¾ç‡: {recall:.4f}")
        print(f"  - F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        
        # æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–
        self.plot_confusion_matrix(cm, category_names)
        
        return results
    
    def plot_confusion_matrix(self, cm: np.ndarray, category_names: List[str]):
        """
        æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–
        """
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=category_names, yticklabels=category_names)
        plt.title(f'{self.model_type} Model - Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(f'{self.model_type}_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š æ··åŒè¡Œåˆ—ã‚’ '{self.model_type}_confusion_matrix.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def plot_training_curves(self):
        """
        å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æå¤±æ›²ç·š
        ax1.plot(self.train_losses, label='Training Loss', color='blue')
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # ç²¾åº¦æ›²ç·š
        ax2.plot(self.train_accuracies, label='Training Accuracy', color='green')
        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='orange')
        ax2.set_title('Training/Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{self.model_type}_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“ˆ å­¦ç¿’æ›²ç·šã‚’ '{self.model_type}_training_curves.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def get_book_embeddings(self) -> Tuple[np.ndarray, List[Dict]]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ›¸ç±ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ï¼ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
        """
        print("ğŸ“ æ›¸ç±åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data, labels, books = self.prepare_data()
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
        self.model.eval()
        with torch.no_grad():
            embeddings = self.model.get_embeddings(data.x, data.edge_index)
            embeddings = embeddings.cpu().numpy()
        
        print(f"âœ… åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—å®Œäº†: {embeddings.shape}")
        
        return embeddings, books

if __name__ == "__main__":
    print("ğŸš€ GCNå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    # GCNãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
    trainer_gcn = GCNTrainer(model_type="GCN", similarity_threshold=0.5)
    history_gcn = trainer_gcn.train(epochs=300, lr=0.01)
    
    # æœ€çµ‚è©•ä¾¡
    results_gcn = trainer_gcn.evaluate_final()
    
    # å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
    trainer_gcn.plot_training_curves()
    
    # çµæœä¿å­˜
    results_gcn['training_history'] = history_gcn
    with open('gcn_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_gcn, f, ensure_ascii=False, indent=2)
    
    print("âœ… GCNå­¦ç¿’ãƒ»è©•ä¾¡å®Œäº†!")
    
    # æ›¸ç±åŸ‹ã‚è¾¼ã¿å–å¾—ï¼ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
    embeddings, books = trainer_gcn.get_book_embeddings()
    
    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
    np.save('book_embeddings_gcn.npy', embeddings)
    
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
